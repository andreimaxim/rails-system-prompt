**DHH**: A couple of years ago, Patrick McKenzie tweeted this wonderful encapsulation about how things are. The fact that at most companies, and most apps, when you delete your data, it doesn't actually disappear. It still lives on in the database, it still lives on in backups, and at most companies will just put a flag on it saying it's no longer accessible, we shouldn't be looking at it again. And I remember reading that and thinking, that's pretty terrible. If you delete your data, it should be gone. It shouldn't be accessible years later for employees to access, to be subpoenaed by someone suing you or the government. The data should be gone. 

And I think that as programmers, we have an obligation to turn Patrick's tweet into a bit of legacy, something we don't do anymore, that we actually do due to due diligence of deleting data for reals. Make it disappear, scrub it from our databases, scrub it from our backups in a timely and reasonable manner. And that was one of the things we've committed to at Basecamp as I tweeted back to Patrick that we have a privacy policy at Basecamp that basically says when you cancel your account, we will ensure that nothing is stored in our servers past 30 days. 

This doesn't happen automatically. This is something you actually have to program. A lot of people who are not in programming perhaps could sort of think, oh, of course I deleted my data, it's gone. No, I mean, in most multi-tenant databases, all the data is intertwined. It's not as simple as basically saying drop database from customers, and then all this stuff goes away. You have to diligently weave your way through the entire web of connections of all the pieces of data that are in an application and cherry pick out the pieces you want to make go away. 

And this is not, as I said, free. This takes real effort. We worked on it for quite a while, and it's pretty diligent work. You don't want to end up deleting the wrong stuff, but you also do want to delete everything that you're supposed to be deleting. So I thought I would show how we do things in Basecamp, because maybe that can give some inspiration to people who are today working on apps that don't actually delete data and just does, as Patrick says, put a flag on it, promise not to look at it again. So let's have a look at that.

In Basecamp, the root piece of data, the root object in this tree that we have is the account. And the account has a concern which we call incineratable. This whole concept of deletion, destroying, these things have specific meanings within Ruby on Rails and computer science and lots of other frameworks. So it's easy to overload those terms and basically say, oh, we're destroying your data and that means something specific. Well, the process we're going through when we're trying to clean up after someone, for example, cancels their account is pretty specific and quite meticulous and has to be tracked in its own way. So we've come up with this term, this notion, this word of incineration, and we're reserving that term to mean this specific thing. The fact that we're shredding all the files, we're burning them after someone has requested that their data be gone.

And I think that this is a good way of making sure that a domain language within your app can have a concern that's not overloaded, it's not delete, it's not destroy, it's none of these generic terms. Incinerate is a term that essentially means the same thing, sort of. But we reserve it for this particular use. And then now when we talk about incineration within the application of Basecamp, it means this one specific thing. I also just think it's a cool word. Incinerate, I just imagine the flames of some burning machine that devours this data, which is exactly the effect that we're after, right? That this data is gone and it's gone for good. But let's look at the specific code that we use to get there.

As I said, incineratable is a concern of the account, it's an aspect of the account. I separate it out from the account object itself, such that we can treat and talk about it in separation. This is somewhat of a recurrent theme that I've talked about in a lot of the previous episodes. This idea that there are different aspects to root objects like accounts. The fact that they can be incinerated. And what we do here is basically just provide a little bit of access around that, such that it's easy to trigger the objects that actually do the work. And in this case, the main thing that the concern sets up is a callback. That after we cancel the account, which is a method that's exposed through another concern, in fact, that handles the account lifecycle, we have this thing to run. We have the incineration that needs to be run.

And the incineration itself is a command pattern, standard command pattern. We pass in the account and then we run the incineration object. But most of the time, we don't actually want to run incineration right away. When someone cancels their account, at Basecamp, our policy is that it will be incinerated 30 days later. So there's basically 30 days of opportunity to regret that decision. You cancel your account at Basecamp and you realize, oh, actually, there was this really one important file that I still needed. Support can still help you out and find that file. But we also do have the option here, this is just the policy, that it's 30 days. If someone wrote us directly and said, no, I want this stuff gone right away, we could through our secondary support levels, just go in on the command line and call incinerate directly here.

But most of the time, this is going to follow a job pattern where the job is scheduled 30 days into the future. And that happens through this incineration job. And I think it's a little unusual, perhaps, that we schedule these incineration jobs far out into the future, actually 30 days into the future. So we have a quite deep queue and long-running queue of these jobs that need to be performed far in the future. We could, of course, always go back and query if, for whatever reason, these jobs were lost and see which accounts are overdue and are due for cancellation. But by default, we have these long-running queues and we just stick them on there and say, in 30 days, run this job. And you can see what we do here, we use this tiny bit of additional syntactic sugar, domain language wrapping in the schedule method, such that when you call the incineration job, you just call it to schedule this account. And then the incineration job itself knows that, oh, it's going to wait for this amount of time before the incineration happens.

That constant, the incineratable after, you could say, shouldn't it live in here, shouldn't it live in the job? Well, we kind of refer to this constant in a few different places. So we've pulled it up and put it inside the incineratable concern for the account itself. But normally, I would have said it should live with the job. So since the job has the responsibility of the scheduling, it should live in there. But since we use it in multiple places, we've yanked it up. And then finally, what all this does is basically just call right back from where it was scheduled, we call incinerate. You'll see there's a little bit of accidental complexity here around signal ID on master. We have a shared database for account information for a bunch of things in Basecamp that we call signal ID. And that has a variety of different databases that we use. And here, we want on the master account or in the master database to run the incineration and then that to be replicated from there.

Anyway, that doesn't really matter that much. What does matter is once we call incinerate on the account, it'll run the actual incineration command, which is where this whole thing starts, right? We pass in the account and then the command pattern gives us two options. We can either run the incineration right away or we can check if it's even possible to run the incineration. This possible check is kind of a second layer of defense. You would say the incineration job itself shouldn't have been called unless you're actually meant to incinerate something. But since we're scheduling the incineration 30 days into the future, things could happen. Things could change. We could have uncanceled the account in the interim. And then that incineration job that's sitting there waiting to run, it's no longer relevant. And we should make sure that the incineration isn't accidentally run just because someone scheduled something if it's not possible to run incineration. 

So you can see that this possibility predicate method is basically just calling two things. First, double check that the account has actually been canceled and that it's inactive. And then make sure that we're within the incineratable time frame, that it has been the 30 days that the policy currently is. This is the example. We're referring to this constant for the 30 days again. And if that's possible, then let's start incinerating. And incinerating, as I said, is this process of walking through the entire tree of code and deleting each individual element step by step. So the first thing, the first step after the account itself, which of course will be deleted last because that's how we go, right? We start deleting all the things that depend on the account and then we end up deleting the account itself once the whole process has concluded. But the first step, the first set of elements we have to delete is the buckets. 

So buckets are things in Basecamp like projects, teams, and what we call pings, direct conversations between people. So we basically run through each of these buckets and then we run the bucket's own command incineration, command object, and go through and do that. And then by the end of it, we go back and we delete the account and we delete the signal account. Actually, as I'm looking at this code, this is a good example of not following a table of contents. Since we call incinerate account first in the order of things, it should be first in the list. And in fact, I would even insert something else here, an extra carrier return, which is something that's a bit controversial, even within the Rails core team, which is to split up, separate the different groups of methods, especially within private, such that the things that belong together go together. 

The do and the cancel predicate methods, they go together because they're part of answering the question of whether this is possible, and then the actual carrying out of the command, the incineration of the buckets of the account and the signal account, they kind of go together. So I like to put in an extra carrier return, RuboCop, which is the style checker that a lot of Rails projects use, usually complains about this. So if you like this style, you have to turn off that feature, otherwise you're going to get a warning. But let's keep going, because as I said, first we started at the top. We start with the account, then we move down and incinerate the buckets, which is the first step that actually has real data, not just metadata. 

The account itself is mostly just metadata, when it was created, who it was created by, and the name of the account. The bucket follows the same command pattern. It has a primary run method, and it has this possibility check. And the possibility check gets a little more complicated. You have whether this is being incinerated by itself, which buckets also can be. I'll return to that. We use the same framework of incineration if someone deletes just a single project or a single team within Basecamp. We lean on the fact that deleting part of the tree of data is a subset of deleting the entire tree of data. So this is a good way of getting some amount of reuse into the system, and being able to lean on the concept of incineration, even within the normal proceedings of the app itself. Not just for counting the cancellation, but for all these other aspects. You could delete a single project. You could delete a message within a project. And all of these steps have descendants. They have children in the tree that you need to diligently delete one by one. So we do that as well. We check on these things, whether the two paths, or either of the two paths, are possible. Can we delete by itself? And if we can delete by itself, that's checking that the bucket, the project, or the team has been deleted, that it's due for deletion, and it's not being incinerated via the account.

And here, it's funny. We have this similar problem, or not problem, but optimization, I could say. We call incineration of the recordings first, and then once we're done with incinerating all the children in the tree, then we can go back and incinerate the bucket. But that should be in the order of the table of content of how the methods are invoked. And I'm inserting also this carrier return, the extra carrier return, to separate the predicates from the carrying out of the command. And as you can see, we keep going here. We keep calling another incineration command object. So let's chase that further down. And now we're getting down into more of the full meat of the actual data. Again, the bucket itself, the project, or the team, is again a bit more of a metadata. It's the name of the project, but it doesn't contain any of the actual data, such as the messages, or the to-dos, or the uploaded files, or any of these other things that we have in Basecamp. But the recordings themselves do. 

So this is where we hit the actual data, the actual content that we're trying to lead. And when we do this, we have a similar check as we did with the buckets. First we check whether we're incinerating through ourselves, or we're incinerating through the bucket. And one of these two paths has to be possible. So the recording is being incinerated by itself, or it's being incinerated by the bucket, or it's not possible to incinerate it. There's some additional complexity here in terms of managing these different stages in the tree. When you're deleting the whole tree, and you're getting into that in a variety of ways, you really don't want the individual children to be deleted out of order, out of the sequence of how you're traversing down the tree. So we have this consideration of whether there's incineratable ancestors, and we just make sure that that's not what's happening here when you're incinerating directly through the recording. But otherwise, it's pretty much the same thing. We go through and incinerate the children, which basically just means that we incinerate all the dependents of this particular recording.

And here's an interesting consideration. The default in ActiveRecord, the ORM, the relation set up with the database, has this feature of touching the parents that something belongs to. So if you're deleting a recording that belongs to a certain bucket, when you delete that recording, we will touch the bucket in such a way that you know that the bucket has changed because its children has changed, and we can use that change order to update caches and so on. But when we're doing this incineration work, we don't actually want each step of the incineration to do this touching, because each touch is an update SQL query, and we don't want to run a ton of those needlessly if we don't need to, especially when you're incinerating a whole account that might have thousands or tens of thousands or in rare cases even hundreds of thousands of recordings in it that all need to be deleted. Doing a touch and an update on each of these is unnecessary. 

So we go through this whole path of deleting, again, the children, the dependents, and then finally deleting the recording. And that's, again, I keep promising this episode on how we set things up with buckets, bucketables, recordings, and recordables in Basecamp. I promise I'll get to it soon. It's kind of a little bit of a beast, so I've been circling around it trying to figure out the best way to explain this pattern, but it is certainly overdue, and my apology for it. The recordable, though, I should just say, is where the actual data lies. That is where the content of a message is. That's where the description of a to-do item lives. And that is sort of the end state. That's the last point in our tree as we walk through and get all the way down to the content. 

So that's a bit of a walkthrough of how things go, all the way from the account to the buckets to the recordings to the dependents of the recordings, using this incineration command pattern all the way down, the same DSL style with run and possible. And as I said, you can invoke this style from anywhere in the hierarchy. So for example, if you're just incinerating a single project in Basecamp, we will basically run from that stage. We have another job that could be queued up from the incineratable concern of a bucket. When that runs, we take it from that step. We just take the bucket, all of the recordings that belong to that bucket, and the recordables that belong to those recordings, and delete those.

What's interesting here about the incineration of the bucket is the fact that we've kind of split it up into two separate steps. We first have the step of deleting everything in the bucket. And deleting is kind of this soft incineration. As long as something is just trashed, which is the first step that we have, the data lives in a visible trash can that people on the account can see, and therefore untrash things that are in that trash can. When they then move to the deleted phase, this is the imminent phase, the imminent status before incineration is no longer visible to the users of the system. But we've put in this final grace period of five days, such that there's a little bit of a safety buffer for someone to discover that the data is totally gone from the system, that they can't retreat it themselves, and they could write support to still fetch this out of the deletion queue before it's gone forever. Because after this has been fully deleted, we're going to no longer have it in our active database. And even the active databases, even though they're backed up, we don't have the power and we don't provide the possibility of having customers fetch things out of those. Because as I said at the beginning, in a multi-tenant system, you have one database with all your customers on it, or at least all the customers between the slices if you have multiple databases for sharding, but you can't really restore those. At least you can't restore those to the production database, as you would roll everyone else back to that stage. So we kind of try to give as many steps as is reasonable, while still honoring the principle of deleting the data when it actually needs to be gone. So that's kind of that multi-step process. And then of course, what we also do is those database backups, they get wiped too. After 30 days, those are gone. So we can't even fetch that data back out. There's no way to get to it. It can't be subpoenaed. It can't be involved in any sort of retrieval process. And it's gone. And I think, as I said, that's how it should be. Data should disappear at least at some point. And I think for us, we've picked 30 days as a reasonable timeframe for data to fully disappear. And this is the code setup that we use to do it.
