**DHH**: Hey everyone, I am David Heinemarie Hanson, I am here with Martin Fowler and Kent Beck to talk about is TDD dead as a topic. Coming off keynote I did at RailsConf and some subsequent articles I posted on the blog, I had a chance to talk with both Kent and Martin sort of one on one in the days that followed and we got to discuss a bunch of these issues and I got to clarify some thoughts and definitions that I had in my head and got a lot wiser about where both Martin and Kent are on the issue and Kent suggested that hey maybe we should share that with the world. So that's what we're doing now. I know a bunch of people have been expecting that this was going to be a big duke out, there will be no boxing, we'll explore a bunch of definitions and problems and patterns we've seen and what's worked for us individually and go from there. But to get started I thought I'd just run over a brief recap of the problems that I've seen with TDD and use that to kick off the discussion. And really three major points that I'll focus on for this discussion. The first is the definition of TDD and its components, specifically unit testing. I know that TDD and unit testing are certainly not the same thing, but TDD is often used to drive unit tests and as I was looking through sort of issues I've been seeing in the Rails world around TDD, a lot of that came down to the definition of unit testing, that people were defining unit testing as it can't have collaborators, you can't touch the database, you can't touch the file system, the goal was to have a suite of unit tests that were so fast that they could complete in a blink of an eye. I've seen arguments of oh if it takes more than 300 milliseconds to complete your entire suite then it's way too slow, I can't use it for anything. And that just wasn't matching my useful definition of unit testing and it wasn't matching what I was trying to get out of it and frankly I wasn't getting anything useful out of unit tests that weren't talking to any collaborators. Second which sort of follows from that is driving your architecture through mocks. If you want everything in your test suite to be so super fast, well obviously you have to talk to mocks, you can't talk to anything slow and you can't talk to too many layers at once. Which leads to a mock sort of heavy and enabling architecture, repositories and hexagon patterns and all these other things that makes it very easy to stop and mock your way out of everything and turn every test you could imagine into a unit test. In MVC frameworks as I work with in Rails, the controller, oh no problem we'll just stuff out everything talking to the view, we're talking to the model and then that's going to be great. I was not seeing that being very great at all, this was the key point where I thought this is test induced damage, that the architecture is actually suffering damage from this drive towards unit testing all the things and that the unit testing all the things is coming through the TDD pipe. And then finally the sort of pivotal point of TDD is this red green refactor loop and that was more of a sort of a personal thing, like we can all have our own personal preferences and how we develop things. In TDD red green refactor seemed less optional and more of a mandate and I never dealt with that for the bulk of the work that I do. At certain occasions it worked and at certain occasions that flow was good, but I felt like there was tons of code that I was writing where red green refactor was just not an enjoyable way to do it and I didn't feel like I was getting to a better place by going through that. So those are sort of the three top line problems that I'd seen with TDD, let's sort of open the conversation up and have Martin and Kent chime in, perhaps take them one at a time. What is a unit test? What's the definition? What's a useful definition of a unit test? Perhaps both what's a useful definition of a unit test and how are most people currently defining it in your guys' view?

**Martin Fowler**: Well I propose that in order to understand something I often find it useful to know how it came about and that the style of unit testing that we're familiar with is basically all Kent's fault, right, because Kent did the frameworks, the ex-unit frameworks, the whole push for extreme programming, so maybe we start with that, where does the source of this style of unit testing come from? You were the guy, you explain it.

**Kent Beck**: There, can you hear me?

**DHH**: Yep.

**Kent Beck**: So when I was a kid I would read books about programming, I was just fascinated by programming and I'm talking about 9, 10, 11 years old and my dad was a programmer so there were lots of books about programming around and he had a book and I would read, I didn't understand anything but I was just sort of compulsively reading these books because I found the whole topic fascinating and there was a book that said here's how you program, you type in the output tape that you expect your program to produce given some input tape and then programming is you just write the program until it produces the output tape that you expected. So that was kind of fixed way back in my brain as the flow of programming and it wasn't until after I had been testing for a while, I'd already come up with the first version of S-unit, the small talk testing framework that was a precursor to J-unit and I remembered this idea of writing the test before writing the code which makes no sense, the test is going to fail and you want the test to succeed so why would you write it when you knew it was going to fail. So I just tried it, one of my heuristics is if an idea is obviously bad, find a cheap way to try it because if it turns out it's not bad then it's really interesting. If you have a good idea that's obviously good, somebody else has probably tried it before. So I just figured I'd try it and I think I built Stack the first time and it worked really, really well for me. I think there's aspects of my personality that make this kind of piecemeal style work really well. I have a lot of anxiety, I tend to get overwhelmed by big problems, I have problems and so for me TDD solves those problems. Even if I don't know how to implement something, I can almost always figure out how to write a test for it and if I can't figure out how to write a test for it, I have no business programming it in the first place. So that's how I got into it and it was definitely ground up. I started with small green problems and gradually worked my way to larger green problems.

**Martin Fowler**: I was thinking about this and reflecting back on our early days together and if I recall correctly when we began working at CFA at Chrysler and using extreme programming as it's now called, we did the testing very strongly from the beginning but it wasn't test first at the beginning. I've got this memory of you standing up there saying, the story's not done until the code's written and the tests are written and the tests are passing. So we had the notion that you delivered the end of an increment of functionality with code and tests together but the test first didn't actually follow until later. I can't remember when that came because it seems to have gradually, it just gradually appeared but I feel that at the very beginning the focus was on the tests, on the tests without the test first as it were.

**Kent Beck**: So I think one of the fundamental questions is as a programmer, do you deserve to feel confident? And I certainly grew up in a culture that didn't encourage that. You know, you write some stuff and you just hope it worked and somebody else would tell you whether they thought it worked and you'd go home on Friday and just think, oh God, I just hope I didn't break something. And so I think going a step back from TDD or automated testing was just do you deserve to feel confident? Can you sleep at night knowing that your code works? I think the answer should be yes for programmers. So if we agree on that then we can talk about how to achieve that. TDD is one way to achieve that but there's lots of other ways.

**DHH**: I think that's a great point because one of the things that really got me into programming was Ruby, right? So I'd been programming before Ruby but here along comes this language that has a very specific mission statement, programmer happiness, right? So I think those two things are related. So the programmer happiness is certainly related to feeling confident in your code and confident in making changes to it and confident that it's going to work but it's only one part of that picture. And the early days that Martin describes were oh, you're not done until you also have tests for a piece of functionality. I'm completely on board with that, right? Where I'm not on board is that that whole feeling of feeling great about your development style, feeling programmer happiness as Matt would always put it, I wasn't getting that from TDD, right? So maybe that's some of the patterns we talk about where different people have different brain styles, right? Like why do some people enjoy Python and why do some people enjoy Ruby and even harder for me to comprehend how do some people enjoy Java but we sort of have different ways of looking at things and then feeling good about them. And what I was finding just from the refactor flow was that the whole notion of writing the test first and seeing that fail and doing that for every piece of code before I move forward was not just a natural because a natural you can overcome just by familiarity. But even through familiarity that it just was not a pleasurable flow and I think that that's where a lot of sort of the explosive debate comes from is to some people writing your thesis first and then filling in the implementation of that works really well. That's exactly how their brain operates. For other people, me included, it doesn't work like that. Like it's a much more tactile experience that I actually have to sit and write it out. That's how I think through it. I don't think through it by proposing the hypothesis up first and then trying to fill it in. I think through it by writing it out. And Kent, as you say, if I can't write a test for it first, I have no business writing it. I totally understand how you can arrive at that. It's just not at all how I feel about it. I have a hard time writing a test for it if I don't have to see it first. So you have sort of these, some people like it from going through the test and other people like it going to the test. I think it's certainly important to realize we're both getting to the same place. Getting to feel confident about the code and confident in making the changes to the code. And that's the part I'm not a fan of tying these two things together. I'm not a fan of tying, oh, you deserve to be confident to TDD. Which I know you're not doing, right? Like TDD is one path of getting to that place. I'm seeing though a lot of people are mistaking those two things. That you can't have confidence, you can't deliver incremental functionality with tests unless you go through this mandated, well paved road of TDD.

**Kent Beck**: So last night was a hackathon at Facebook and I have a project on an internal tool that I was working on. And half of it I could use TDD for and half of it I couldn't. So I had a big log that I wanted to process and the entries in the log were not clearly specified anywhere. But I wanted to put them into a data structure. This kind of funky new data structure. The data structure I could build with TDD just fine because I could see, I could break it down, okay, here's the main case, here's the corner cases, you know, here's how I go from one to many. I have this nice sequence of tests and I could get into a real flow. Just make one test work and the next one and the next one and the next one. So that feels a certain way to me. When I had to feed live data into it, it feels a very different way because I don't have that clear specification. I just have to run a few million log entries through this first and see what blows up. So I put in logging, I put in exception handling so any log entry that's kind of funky that isn't handled by the code gets saved some place and then I can go back and look at those entries that I can't handle yet and I can use them. But I can't write a test for it because I don't even know what the inputs look like. And that feels a different way to me. So with the parts of the code that I could use TDD for, I'm in a real flow, I write a test, I make it work, write a test, make it work, I look it up and it's 1230.

**Kent Beck**: So I really like that feeling. The ones where I just have to run on live data and see what happens, I feel a lot more anxiety. But it's called work for a reason. So I'm going to use the principles of double checking, of regression testing, of short feedback loops, and try to get the shortest possible feedback loop out of this unpredictable input that I'm getting. So I had to mix the two styles in one night. And it doesn't bother me. I think if I can play classical and jazz, then I'm in a better position to make music with more people. But when I can reduce a problem to this sequence of tests, derive the solution from a sequence of tests, it has a special kind of feeling for me that works really well. If I think back to how I learned discrete math, I always used examples. There'd be some general prove this statement. And I think, well, what's an example of that? And then I'd come up with a few examples. And I'd convince myself either that the statement was true or that it wasn't. And then I could write the proof. But that's my personal flow. I work from specific to general, this kind of inductive style. And I understand not everybody's like that.

**DHH**: What I think is interesting, though, is even if I've been in that flow, too. I have been in a flow where TDD was working out well for me. And it was often, as you described, where there was a very clear input and there was a very clear desired output. And there were not a lot of context. There was not a lot of environment to operate in. It was very pure. And for that, that would probably be the one case where I do actually like that flow. What I find is, A, a lot of my work is not like that. So it's sort of fundamentally or innately not like that. And then the problem comes in, what am I willing to sacrifice to get there? You can reduce any type of work, certainly in the web space that you're doing, to that flow. That's where the whole mocking thing comes in. If you just find a way and work hard enough, you can remove all the dependencies, all the environment, and reduce it to just one piece of input and one piece of output. Is the flow important enough that you want to do that as a general case? How much are you willing to sacrifice to be in that flow more of the time for more of your work? For me, the answer has been, I'm not willing to sacrifice that much. When the flow is natural, when the work is almost of algorithmic nature, and it's separated from all this content, all right, great. Let's switch into that mode. But forcing all sorts of other kinds of programming work into this mode, just such that we can feel the TDD loop, that's where I really see that we're going off track. And just coming up with some, in my opinion, nasty hacks. And we start making really bad trade-offs for other aspects of the design of the code and for the productivity of the code. So I'm curious for both of you guys, how much are you guys willing to sacrifice to get to that TDD flow? Are you willing to use heavy amounts of mocking? Do you find that generally you can get there in, say, MVC-style programming? Or what are the boundaries for you guys?

**Kent Beck**: Yeah. Martin's waiting for me to talk, so I guess I'm talking. So as I see it, I do this all the time. It's a question of trade-offs. And my students have seen me do that 1,000 times. So for me, TDD is a question of trade-offs. And I think, David, you point out an important part of the trade-offs, which is, to make a design's intermediate results testable comes with a cost. It comes with a benefit, too. And you've got to figure out whether the costs outweigh the benefits. Sometimes, that's a question of understanding how to design things well. So if I'm building a compiler, and I think, well, the only kind of tests I can have are end-to-end tests, and then I start building more and more cases of what the input looks like. And I realize, oh, if I have this intermediate representation of a parse tree, then now I can test parsing. Now I have these two orthogonal dimensions. I have parsing. And then, given a parse tree, does it compute the right results? And then I can do more testing at a finer grain. But it was a design insight that created that moment. And compilers have settled on that as being a reasonable trade-off. OK, it's worth having this intermediate representation so that we can have testability and a bunch of other stuff that comes along with it. So I'm not willing to say, OK, there's some fixed boundary, and I don't want to twist the design just to make things testable. Because I always figure, ah, I'm just missing some design idea. And if I had a better idea for the design, then I could have it both ways. I could have something that was a more aesthetic, more flexible design, and it was more testable at the same time. But if I don't have that today, David, I think your point is exactly right. Then what do you do? Do you mock absolutely everything? My personal practice is I mock almost nothing. If I can't figure out how to test efficiently with the real stuff, I find another way of creating a feedback loop for myself. I have to have the feedback loop. And the feedback loop has to be repeatable. But I just don't go very far down the mock path. I look at code where you have mocks returning, mocks returning mocks. And my experience is if I use TDD, I can refactor stuff. And then I heard these stories. People say, well, I use TDD, and now I can't refactor anything. And I couldn't understand that. And then I started looking at their tests. Well, if you have mocks returning mocks returning mocks, your test is completely coupled to the implementation, not the interface, but the exact implementation of some object three streets away, of course you can't change anything without breaking the test. So that, for me, is too high a price to pay. That's not a trade-off I'm willing to make just to get piecemeal development.

**Martin Fowler**: And this is, I think, at the heart of much of this is confusion over terminology and what these different things are. When I read David's initial blog post, because I didn't see his talk until last night, one of the things that came through very clearly was his criticism of TDD and of the design damage that flows through it had, in itself, very much tied in a notion of this strong desire for isolation and mocking. And there, it's very important to point out that there is nothing sort of within the idea of how you do either TDD or unit testing that says you have to have that kind of isolation. Some people are very much in favor of it. Others aren't. And I remember in the early days of extreme programming unit testing, people would criticize us for saying, hey, you're not isolating your units properly. That's not unit testing. And then we had this whole conversation with 24 different definitions of unit testing, or whatever it was, and said, well, our style of unit testing, we don't bother about the isolation. And it's working well for us. Thank you very much. So that's one thing, whether the TDD and the unit testing should be tied in with isolation. And I look at it as there's different schools of thought. And I'm with Kent. I hardly ever use mocks. But I know good people who do, so I don't want to shoot everybody who uses mocks. Maybe give it 10 more years, and then we'll drum them out or something. But we'll see. Then there's another thing, which is a distinction between having what I call self-testing code and TDD. To me, it's really important to have self-testing code, the ability to be able to run a single command, have your whole system self-test itself in an acceptable amount of time. That is really powerful. Because then if you can do that, you can refactor with confidence. You've got a good chance of keeping your code base healthy. And that means you're able to be fast, deliver new features, et cetera, et cetera, et cetera. That is a really powerful thing. And if I want to get on a high horse and say you must do something, I might be inclined to get on that particular high horse. But for me, at least, TDD is one way to approach that. TDD is a very particular technique. And if done well, one of the benefits is it gives you self-testing code. Gives you other benefits as well. But self-testing code is one, and for me, perhaps the primary benefit. And the two get conflated. Now fundamentally, if somebody gets self-testing code by another route, such as when we started and we were delivering the code and the tests together at the end of each iteration, and you've got that self-testing confidence, then I'm actually pretty happy. I'm not going to feel upset that somebody got there by another route. And I share with David and Kent, there are problems where TDD doesn't work terribly well. And sadly, most of the programming I do these days is not conducive to TDD. And I miss it, because actually, I really like the TDD flow that really works for me, just as it works for Kent. But fundamentally, we have to separate TDD from self-testing code and realize they have different benefits that often get conflated together. And we have to separate the idea of fully isolated unit testing from not isolated unit testing and realize that people have different preferences around those.

**DHH**: And that's where I see that was what much of my reaction was against, right? That we had this prevailing definition of TDD as not from you guys or even from Kent, but from the people currently speaking in the programming environment that I found was A, very mock heavy. B, was very moralistic about the specific technique to get to the place we all want to get to. We all want to get to feeling confident about making changes to the code, having a self-tested system that can run as a command and all these things. But you couldn't be part of that boat unless you also signed up for the TDD road to get there. And that's where I found it really frustrating, that we could have talks about whether we were being professionals or not. And that went through, well, are you writing your tests first or are you writing them second? And related to that, that the whole TDD notion was propped up as, oh, well, that self-testing code you get? Oh, that's just a byproduct. That's a side benefit. That's the leftovers from driving your design through the tests, right? That a lot of people really hinged on to this idea that TDD was not about self-testing code. That was just a nice two-half. But the key benefit was the notion that you could only improve your design by making more testable. Those two things equated, that an easier-to-unit test system was a better design system. And that's the main sort of the heart of the fallacy that I want to take a real big swing at. Because I started seeing a whole lot of code where this was not true at all. The code was not better. It was not better designed just because it was more testable. It was full of these three streets of mocks that you guys are talking about. Because we were trying to apply the unit test paradigm to areas of the code that really didn't fit it very well. And we were discarding all sorts of really useful techniques, really useful patterns, like active record, because it didn't fit this notion of unit tests that had to run in 300 milliseconds, couldn't talk to the database, and thus wasn't isolated. So we had to throw out all this productive work just so that we could sort of worship at the altar of a test-driven design and that every aspect of our system could be unit tested in isolation. That's where I think we needed a reboot. And I think we still need a reboot. We haven't rebooted enough. Like, these things are still intermingled. And while I think we're all very much on the same page of trying to tease these things apart, most people are throwing it all into one big pot. They're calling that pot TDD. And TDD means self-tested code. It means red-green refactor. It means mocks, because you have to unit test everything. It means this big ball of shit, in some ways, because I think that that's what comes out of it. The dish, in the end, is just not tasty. The code is not great. There are other, more important, sacred principles than just, is this testable? As in, is the system clear? Is it understandable? And all these other aspects of it. So that's where my frustration really is.

**Kent Beck**: So I get to play time cop now. We said we'd do 30 minutes. 30 minutes has come up. And it seems that we'll do another one of these Hangouts at some time in the future, but we haven't decided yet. But I think one way we could immediately begin is to explore this question. In what ways can a desire to TDD damage an architecture? And is it damage? Some people say this hexagonal rail stuff is much better than that crap DHH is imposing on us. Is that the case? How do we evaluate it? Those sound like good questions to tackle in the next episode.

**DHH**: All right, thanks a lot.

**Martin Fowler**: Thank you.

**DHH**: Thanks, everyone, for checking in. We'll set up a new date and shoot out another link. That's great. 

**Kent Beck**: Bye-bye.

**Martin Fowler**: Bye-bye.
