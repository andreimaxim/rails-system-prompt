Kent Beck: Welcome to the third episode of Is TDD Dead? I'm Kent Beck from Facebook and I'm here with the David Hanemeyer Hansen and Martin Fowler and we're going to follow up on episode two with a conversation today about the trade-offs inherent in testing. I think of most interesting questions in software development as involving some kind of trade-off and I do this with my hands which I understand is reminds some people of some sort of bizarre form of modern dance which is your problem not mine but what I mean to convey with that is that there's there's some set of costs on one side and another set of costs on another side and there's some minimum between those two curves and depending on the way that the two cost curves are shaped will change how exactly you decide to to go on some continuum almost no questions in computer science can be answered A or B. The answer always begins with it depends and what does it depend on is the interesting part. Now in some ideal world we would have instant infallible feedback about our programming decisions so every keystroke that I make if the code is ready to deploy it would just instantly deploy within milliseconds so if I just happened to create some software that works then boom it's instantly out to everybody in and that kind of feedback is impossible at the moment. By calling my methodology extreme programming I learned you can't ever say never because what seemed extreme at the time turned out not to be but that that's the ideal world that you get this instant feedback about whether your software is ready to go or not and the reality is that we have to back a ways off of that. The question I think is how far off of that do we need to back? Some of the some other constraints in my mind as I'm wondering about this this question is how frequently do we need feedback and we've had a range of answers between the three of us we've had a wider range of answers on on the fabulous interwebs about ranging from 100 milliseconds to minutes certainly so how frequently do you need feedback about some decision you made is it doing what you think it does. A second continuum is fidelity so in some ideal world just be red and green you know and as programmers we're comfortable with that sort of binary thinking in fact any set of tests is going to be accurate only to some number of decimal places so so if the tests are green and we push to production some percentage of the time everything's okay and some percentage it's not so how how much are we willing to invest to get better fidelity in the tests and that's a continuum and certainly different kinds of software require different different levels of fidelity and I think different personalities require different levels of fidelity another thing that I think it we've been dancing around and haven't really addressed directly is overhead so as we change some of these other levers say if you want to increase the fidelity of the testing of your test suite the cost of that's going to go up at the same time so how much of that overhead are you are you willing to accept to get more frequent or higher fidelity feedback and last of the major issues that I identified is lifespan how long is the software going to live I'm often in the Facebook has this tradition of hackathons and going into Facebook I had a pretty fixed here's how to program and I realized that that style with with absolute confidence in the in the quality of the code and so on well you know of my hackathons none have ever shipped so I've participated in I don't know 10 or 12 hackathons none of that code is still alive so how much effort did I really need to put in to finding out that my idea wasn't worth pursuing I had to write the code but in this this question of overhead well if I wanna dial the overhead way up for code that's just gonna be discarded because I wrote it to learn something I learned it now I throw it away it doesn't really make sense so again there's a continuum of lifespan and and lifespan isn't just measured in years it's it's measured also in probabilities this is really likely to live a long time this is really likely to to die very soon so that's how I think about the this trade-off when somebody says okay what style of feedback do you want to get from from tests these are all the constraints I think well okay this isn't gonna live for very long but it's a really complicated algorithm and and and so therefore I'm gonna test about this much and here's where I'm gonna stop trying to think about about crazy cases and then I'm in some other situation you know it's a load balancer you know extremely high performance and the curves go completely different way so I I don't know if that if that helps to clarify the differences I mean we're we're not in this to agree my personal goal is just to understand the the set of trade-offs by having to articulate them to people who are prepared to tear my ideas apart in a constructive way and I'm so I'm I'm not aiming for agreement but that those are the things I'm thinking about as I'm thinking about a a style of testing. 

Martin Fowler: Is that jibe with what you guys consider different? Am I missing something? Well to some degree but one thing that occurred to me as I was thinking about that closing question from last time is well there's more than one thing that you're looking for feedback on I mean to and one of the most important parts of feedback is is this software doing something that's useful for the user of the software and that's something that sometimes can be expressed very nicely through tests I mean that the classic you know here's the payroll here are various example cases work through the tests works beautifully on the other hand I'm figuring out how I wanna render my overview page of is TD dead on into HTML and I'm not actually quite sure what I want as an output until I try a few things and look to it I want feedback and I produce that feedback by looking at the page, seeing how it feels, clicking around it's not something that's conducive to tests which is a point that you know even even Uncle Bob has made if you're playing around with the user interface and you're trying to get that sense of what's it like going to be for my hopeful user then that's not going to fit so one category of feedback I would say is thinking about your users needs and how you're going to satisfy them and a second category is have I broken anything now I'm putting in this new little feature here have I broken something over there and that's where the self-testing code notion the regression suite is such a lifesaver I mean to be able to run the tests and go okay I don't have to worry about stuff breaking elsewhere is such a magnificent plus that also ties into the whole issue of isolation and modularity and stuff, I'll put that to one side slightly but that's another form of feedback you want, you want to know you haven't broken anything and that's where for me the green bar red bar is the real kick I actually don't care about the green bar red bar for the new functionality because I want every test to fail even if I'm doing TDD before I see it succeed but or at least I need to see it fail at least once one way or the other but that green bar red bar is vital only have I broken anything and then the third dimension I say that is my code base healthy, is this a code base I'm going to be able to continue to run quickly with for the length of time I expect to be working on it and that's a much more tricky thing to get feedback on I mean you do, you build your aesthetic of how to name things, how to organize your code but it's still a questionable thing and it gets even more difficult if you know that code is going to be taken up by other people potentially even people you don't know yet, I mean that's the classic problem the ThoughtWorks teams face, they go in they're doing a project, they don't even know who they're going to hand the code over to at some point and so that really complicates the is the code base healthy aspect of the feedback so that's three forms of feedback I can think of user needs, have I broken anything, is the code base healthy and I think that that's part of the discussion that goes a little broader than where a lot of the TDD discussion so far has centered on which has been on these practices and on the sort of the style of the unit testing or whatever and it's very much just talking about oh the programmer working on the code and what can he do for feedback.

DHH: One of the things that we found or rediscovered I guess was before we had TDD and before we had programmers writing their own automated test suite we had a lot of QA people and then TDD actually in many ways became so successful that a large number of shops they cut out their QA people, they don't have people doing QA anymore Basecamp was one such shop for quite a long time, it's only two years ago I think that we started having somebody work QA which by historic standards is an anomaly like most software otherwise produced would have a QA department and I think part of TDD being so successful in giving programmers confidence it gave them overconfidence I think it gave them overconfidence in the quality with which they can produce software through testing that they come up with as part of development and then they got so overconfident that they thought they didn't need QA and then they started shipping software that on not all parameters were better quality I think they're obviously the old model was broken right like just never testing your own code throwing it to a QA department you don't give a shit about the quality, it's just going to be a royal mess for them because you know somebody else is going to clean up after you so that's the pendulum swing to one extreme and I think to a large extent we've swung too far back to the other extreme which is oh well, I'm a programmer, I now have tools, I have TDD, I have all sorts of infrastructure that can make it so that I can test myself I am sufficient, I can produce good software on my own accord I don't think that's true at all, I don't think you can work on anything of material quality and produce great software without having somebody who's not you test it, and I think that it's disappointing to me that that sort of drifted out of the conversation and the reason it's disappointing to me is I've seen just how powerful somebody who's not part of a real QA person can be somebody walks in not knowing how I implemented things, somebody who's not thinking about my happy path somebody who's just thinking about breaking my stuff before the users do so I think that's one part of it, the other thing I wanted to focus on was to do this, the trade-off part you have to know what the cost is, you have to know what the downside is, you have to know what the drawbacks are and I feel like it hasn't actually been this when it's been TDD and testing, it's just been this like there's been one bar which is the positive bar, this is oh let's make the test run faster minutes are better than hours, well great, let's do that well seconds are better than minutes, well let's do that, well milliseconds are better than seconds, let's do that but you never got this back, right? if you just keep chasing this and you just keep chasing improvement, I think you can become blind to what the original intention with trade-off was, was that you were trading something off of value you're giving something away that was valuable you didn't just want to give away, it cost you something, it either cost you time it cost you complexity, it cost you overhead, it cost you all these other things, right? and I think that that discussion

DHH: Where I'm seeing that that's missing so distinctly is when I say stuff like test-induced damage, I see this look in some programmers' eyes just going, what the hell are you talking about? This is not even conceivable notion to me. I cannot even comprehend the fact that you could introduce damage through testing, because I only see the one thing. I only see more tests or better tests, faster tests or better tests. So we just keep pursuing more, and we keep pursuing faster, as though that was the only sort of, it's only one part of the equation, right? So that's why I think we have to drag it back. And to get to this, we have to understand all those drawbacks. And I think that that's where I think you're spot on when you say it's a continuum. Because this is true in everything. So operations, for example. When we first launched Basecamp, I was kind of pleased when we got to 98.5% uptime. That sounded impressive to me. Then at some point, we got to 99%. And I'm like, hallelujah, this is amazing, right? And today, we're at five nines. The cost of going from 99 to five nines was exponential, which is the same thing. That's how I feel about tests. To get to, say, 99, five nines of coverage, it's going to be exponentially more expensive than to get to 99. And even getting to 99 is going to be so much more expensive than getting to 80. One of the terms that stuck in my mind when I read the first time was this notion of criticality, where if you're working on a NASA space launch module, you better chase 99.999, right? If you're working on pacemaker software, you better do that. If you're working on gluing some services together for a web app that's just launching, that's just exploratory, well, no. You should not do five nines of reliability. You should not do five nines of coverage. And I think that you can't have that conversation while at the same time having, at least from some quarters, the notion that you should not write a single piece of production line before you've written a failing test. Those things don't go together, right? You can't, on the one hand, claim, well, we need 100% coverage because that's our design methodology, and then at the same time also feel like, oh, well, we're doing a trade-off that depends on the criticality of our domain. So anyway, I think that that's where it's really interesting to talk about trade-offs. The interesting part is to talk about all the things that you're giving up and whether those things are worth giving up.

Kent Beck: Yeah, I want to get back to what you were saying about QA and now the pendulum has swung too far. And I think about it a little different. First, that old school, now I hate to play the age card, but I worked with QA departments enough times to see the dysfunctions that came about as a result of that. And the big one, the only piece of Facebook swag I have in my office is a big poster that says, nothing at Facebook is somebody else's problem. And the remarkable thing about Facebook as an engineering organization is the degree to which 7,000 people all actually agree on that. I've never seen an organization this big take this much responsibility. Getting rid of QA, and Facebook, until very recently, had no QA at all beyond programmers in the traditional sense. Now we have some people working on the mobile apps. But it's very programmers take responsibility. And it works OK. It's a question of compared to what? If you said, well, if you had a really healthy relationship, if as a programmer you still took responsibility, but somebody else was helping you be even higher quality, wouldn't that be better? Sure, if that's what you're going to compare it to, then absolutely. But the transition back in the dark ages wasn't from a healthy relationship. It was from this very dysfunctional, irresponsible attitude, throw it over the wall, long, long cycles, vague feedback. Going from that to programmers accepting responsibility for the quality of their work, that was a huge step forward. Can we go beyond that? Sure, absolutely. But I don't even see it as a pendulum. There's some level of complexity in the platform you're on, or the complexity of a product. You get over some threshold, and a programmer can't keep all of the important possibilities in their head, or doesn't yet know how to design in such a way that you can keep all those in your head. And then having external feedback about the functional quality of software certainly makes sense. But it's a continuum, and it's a set of trade-offs. And I'm quite happy in most environments to accept the responsibility that my code works, and I don't need a net, and I work better if I don't have a net. So just in response to that one, Martin, did you have something to say about the rest of David's response?

Martin Fowler: Actually, I was also going to follow on from that. We can move on to the second one later. We at ThoughtWorks, pretty much all of our projects have some form of QA present. I mean, that's pretty much always been there right from the beginning, partly because enterprise is expected, partly because of the sizes of our teams are often bigger. The shift that has really struck me from when I think of QA departments in the 90s, as well as this toss it over the wall, and an encouragement of this adversarial relationship where QA are there to catch the programmers out, poke sticks at them. And the programmer's reaction is, oh, who gives a toss, because it's quality is your problem anyway. That has changed. It's much more collaborative. The other shift, as well, is a hell of a lot of what QA people were doing was just mindlessly going through test scripts. And the automation has really got rid of that. And as a result, you can cycle much faster. You can say, oh, we can get this code. We can put it into a deployment pipeline. And we can get some answers in hours, at the worst case. And we can then go and deploy from that. It's not a case of, well, the QA department's going to take a week to go through those changes. So that's, I think, from our perspective, the big shift. And the fact that it's also let a lot of startups be able to run for a while without any of that, I think, is a huge liberating piece, as well. So that's how I see the shifts in what's happening with QA.

DHH: I think that's great, as long as that's a mindset that you have to it, that we're willfully trading off some degree of quality for the time being, because we want to run fast, break things, and maybe we're not going to be around tomorrow. What I see is that people have internalized the notion of programmer testing to such a degree that they don't even recognize the value of exploratory testing, other than programmer testing. That's not everyone. But I feel like there's a generation of programmers who came up not knowing the battles of the 90s. The only thing they ever knew was programmers responsible for quality. They knew quality is meaning tests, automated tests, which is a, perhaps that was better. Not even perhaps. That was better than what it came from, right? But it's also still a shallow understanding of what quality is. And I think it's part of those sort of blinders that you have on, that if you consider, well, I can create software that's completely of high enough quality, just on my own, without any QA, without any exploratory testing. I mean, my answer would simply be, no, you can't. The only reason you think you can, and do you know why you think you can? Because all your tests are fucking green, but they're not finding all the problems, all right? Like, as soon as you push out your code into the actual real world, users will start doing, why are they doing that? Why are they uploading this file, or a folder, or something else that they're not supposed to do? Like, nobody reasonable would upload a 2 gigabyte file into this field. Like, I didn't account for that. I didn't account for the fact that the moving on the back end will then take 50 seconds, because I only tested with 5 megabyte files, right? There's this huge dark room that all of a sudden is switched on. Worst of all, though, is when it's not switched on, which is what happens when customer service does not relay the atrocities that you've pushed out into the world, right? That's the kind of feedback that a lot of people are cutting off from themselves. Like, I've dealt with programmers for 10 years at Basecamp, where we've had this sort of inherent conflict that programmers didn't want to be on call. They didn't want to deal with customer complaints, because that's kind of drudgery work. But it's also the information feedback loop, right? It's the feedback loop that goes beyond just, I wrote my own test. I'm designing my own feedback loop. Here's the actual real world flowing into that, right? And that's often awkward. People don't want to see that, right? I can understand, in the same ways that TDDs, perhaps in the early days, taught people, like, here's this extra feedback loop. And people didn't like what they saw, right? Like, holy shit, my code actually is quite poor just when I'm testing it myself. So now you need to reach a level of proficiency where you can create code that you yourself can test and can be green. And that can feel good. But it can also be a plateau, where you don't realize that there's many, many steps above that in quality and sort of goodness of the program that come from actually users still find problems, right? And that's the split of like, oh, I have 100% coverage of my code, yet I still have bugs, like there's some cognitive dissonance that's going on there, where it's just like, how is this even possible? Like, this is not supposed to be possible. All my tests are passing.

Martin Fowler: Right, and maybe we should always stipple a few red pixels in every green bar as a reminder not to be too arrogant about just what that means. And the on-call is the feedback loop that teaches you what tests you didn't write. Sometimes you can capture on-call things as tests. Sometimes you can capture them as alarms or the need for more logging or the need for gathering more statistics. But it teaches you what feedback loops you don't have automated in place yet. And yeah, I'm with you on that. Facebook on-call is, programmers are on-call. And it's not pleasant work. And everybody complains about it. And there's no way that we're going away from it because it's just so valuable as an engineer. I mean, as soon as you think that you're not making mistakes anymore, you're not willing to accept the possibility that you're making mistakes, then you're making a mistake. And you stop growing as an engineer. And that'll take you to some level of complexity maybe. And then the world won't let you pretend that you're not screwing up anymore. So I'd rather catch that early. And if that means that I have a phone going off by my bed at 2 o'clock in the morning because some configuration of some load balancer got changed, da-da-da-da-da-da, and now my service is crashing. And there's a lesson for me to learn. If that's the price, that's the price I'll pay.

Kent Beck: So talking about prices, we are now at cutoff time. We didn't get to the second part much of David's point about costs. So is that something we should look at next time, look at the costs of a testing world?

Martin Fowler: Sure. And I feel compelled to plug the fact that this guy Mike Bland, who's an ex-Google guy, has been writing an article on my website and just published today, by coincidence, an installment on the costs and benefits of a testing culture. So maybe that could be a bit of input to it. And if not, we can riff on how we go next week or whenever it's going to be.

Kent Beck: Down again. And again, we'll announce on the feeds when the next Hangout is. I've also got a regular page set up now that always have the next page and the links to all of the previous ones as well. So that should make things a little bit more organized.

DHH: Over time.

Kent Beck: Over time. We're programmers after all. Yeah, the next piece of organization will actually be to decide the next Hangout before we begin this one. That's my stretch goal for the future.

Martin Fowler: Yeah. Sounds good.

Kent Beck: OK. So bye bye.

DHH: Thanks, bye bye.
