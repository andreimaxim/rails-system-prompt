DHH: Hey everyone, this is David Heidemar Hansen. I am here with Martin Fowler and Kent Beck to talk for the fourth round of Is TDD Dead? Today we're going to talk about the drawbacks, the cons, the pressures, the nudgings of TDD, basically the other half of the trade-offs. We keep talking about how TDD and other testing techniques are all about trade-offs. So to talk about trade-offs, you really have to understand the drawbacks. If there are no drawbacks, there are no trade-offs. And I think that that's perhaps one of the things that I've been most interested in with this whole debate, is to basically illuminate the fact that things are not free. Nothing is free. No technique is free, no approach is free, and TDD surely is not free either. And we have a number of these pressures and nudgings to talk about. That's, I think, the paradigm to think about. TDD doesn't force you to do anything. Nobody forces you to do anything. I feel like TDD nudges you in a certain direction, or it exerts a certain amount of pressure to follow a path. It doesn't mean that it dictates that path. It doesn't mean you have to follow that path. That's just what I've observed watching people do TDD, and watching myself do TDD at times. And I think the first thing that I spot, that I noticed, relates to over-testing. One of the ideas that is sometimes espoused along with TDD is this notion that you shouldn't write a single line of production code before you have a failing test, which basically means that every single line of code you write, you should have to test first. At least for me, at first that sounded very reasonable. The effects that I saw from that were a lot less reasonable. I saw that sort of come out as a lot of over-testing, as a lot of focus on almost a ratio chase, where having four lines of test code for every line of production code was seen as a good thing. And that really bothered me, even from the get-go of getting into TDD. And I didn't quite sort of put my finger on it until I started working with systems that were developed that way. And the whole notion of having multiples of test code to single lines of production codes, to me, is often not a good thing. The over-testing that that leads to makes it harder to change behavior. Because to change the behavior, you have to update all that code. Even though it's testing code, it's still code. It still has to be kept in sync. And while sort of a BDD style, or behavior-driven style, is an improvement over just testing methods, it's still stuff that has to change when the behavior changes. And I find that that's part of the thing that's sometimes swept a little under the rug, is this notion that, oh, well, you're writing all these behavior tests, but so they shouldn't have to change just because you change your code. Well, that's true, if all you're doing when you're changing your code is refactoring. That is not changing the functionality when you're updating the code. But yeah, that's some of my code changes. But far more of my code changes are actually driven by wanting to change the actual behavior. I want the system to do something else than what it's currently doing. Sometimes that's new behavior, but a lot of the times it's also changing existing behavior. And if for every line of production code I want to change, I have to change four line of testing code, there's something that's not right there. And I think that sort of where is the right point, what is the right ratio, comes a lot down to what we talked about last time, which was this sort of the nines, right? Like some code is worth five nines of coverage. And other code is maybe worth 80% coverage. Some code is maybe worth 60% coverage. I'm not hearing a lot of sort of support for that. I mean, a lot of support for every line of production code should have failing tests, which almost guarantees that you're going to write far more test code than you're going to write production code. One of the things I've seen specifically in something like Rails is that we take declarative statements. For example, in ActiveRecord in Rails, we have something called validations, where you can say, oh, this attribute, it must be present, otherwise it's a failure case. And I've seen people write maybe five line of testing code to test that one declarative statement. And they write that first as a unit test, and then maybe it's also included upstream as a functional test, and maybe it's also included somewhere as a system test. And I just go like, wait a minute, this is not worth it. And I don't think you get to that. I don't think you get to that trade-off before you start thinking about, when am I over-testing? When is this piece of production code actually not worth writing the test for first? So anyway, let's open the ball with that declaration and hear what you guys have to say on that. Have you seen over-testing? When do you see over-testing? One of the quotes I've used from you a couple times is this notion about confidence, that you're not paid to write test code, you're paid to write production code. You write just enough test code to be confident in the production code you test. Or you write, which presumably means you don't actually write a failing test for every single line of production code before you write that production code.

Kent Beck: Well, it depends, and that's going to be the beginning of all of my answers to any question ever. I think that's interesting. So for example, JUnit had 100% coverage, and we followed this. No functionalities written without a failing test first, very strictly from the beginning. And we were really happy with the results of that. Now, we didn't end up with an over-testing situation. I think that's something different. I was curious about your statement that it seemed like to me that you were saying that if you follow that kind of a discipline, you're going to end up with too many tests. Now, I certainly believe in the idea of too many tests. My cousin, Herb Derby, came up with this metric of delta coverage. So if you look at your test suite and you measure what coverage each test adds uniquely that no other test provides, if you have a test suite and you have lots of tests with no delta coverage, that you could just delete the tests and not lose your ability to exercise parts of the system, then you should delete those tests unless they have some communication purpose. So I want to pay attention to that. Like, am I testing something unique with this new test? If you aren't, then you need to think about whether you really should write that one, if this test shouldn't be written, or if the other test that covers the same stuff could be deleted. I oftentimes do that, especially with tests that I write early. I call them scaffolding tests, where I'll write some systemy kind of test. I'll get it to work in some really janky way. And then later on, refactor, refactor, refactor. And I look at this test and I say, oh, well, nobody actually really cares about the zero input case. So I'm going to delete that test. And people freak out, like, oh, you can't delete tests. But of course you can. And if it's not buying you anything unique, you definitely should. So I think it's a question of coupling. If you have the exact same behavior covered multiple different ways, then you've got coupling. Because if you change any one of them, you're going to have to change the others. And coupling comes with costs. I was about to say it's bad. It's not bad. There's always going to be some coupling. But coupling comes with costs. And you better get benefit for it. So if you're just covering the same stuff three different ways, just because you think that's a rule somebody made up someplace, then yeah, definitely don't do that. Martin, do you run into over-tested code?

Martin Fowler: I haven't looked at it directly. I'm sure it happens. And I'm sure if anybody's going to produce over-tested code, it's probably ThoughtWorks. Because we have a strong testing culture. And when you try to do anything well, you're not going to hit the line dead on all the time. Sometimes you're going to undershoot. Sometimes you're going to overshoot. So I would expect us to overshoot. And that would be, I wouldn't be worried about that unless the overshot is too large. As to what causes an overshoot or the habits of an overshoot, that's not something I've really looked into and examined. When it comes to this issue of every line of code should have a test failing for it, there's a couple of things I tend to look at. One thing I do do is I always ask the question, if I screw up this line of code, is a test going to fail? So maybe I've got a conditional in the line. And if I reverse it, or I'll use greater than equals instead of greater than, or maybe commenting the line out. And I'll actually do that. I've actually done that and gone up to some code and said, have I got a test that covers this? Well, just comment the line out and run the test and see if anything failed. But as long as one test fails, then you're OK. Because a double check only requires one test to fail. And it might be indirect. I mean, that test might be testing something that's something that's using the line of code. And because I screw it up, then that fails indirectly. I'm cool with that. It doesn't have to be super precise. But that's the kind of mental test I use. And then the other mental test I use is your old one of only test things that could possibly break, which is simple getters or something like that. I'm not going to test that. Now, when we come into things like some of these declarative Rails things, I actually don't know how far I'd take that. I mean, unless I really got some wonky framework or library, I'm going to assume the library works. But does something that I'm trying to put in there, is it something where I feel I could be messed up? And then again, what are the consequences? If I do mess this up, what are the tradeoffs? And that lies into David's point about the criticality. Now, if it's not a big deal, then I'm going to be less concerned about it. But I would say my biggest test is this, should I come into line out, or should I reverse an operator or something? If that situation, is some test somewhere going to break? And that tends to be more my mental test.

Kent Beck: I think this ratio of test code to functional code is a bogus metric. I think that's more a consequence of coupling in the code. So a huge formative experience for me was watching a compiler guru named Christopher Glazer write a compiler. And I was already kind of bought into the tests. But he literally had four lines of test code for every line of compiler. And his code was absolutely rock solid. And that opened my eyes to just how much effort can profitably be spent on testing. Now, in a compiler, you've got tons of coupling. You can make this innocent-looking change over here in the middle pass, and then something breaks over here in optimizations. That kind of stuff happens all the time. So if your system has lots of that kind of coupling, that ratio should go way up. If you have a system that's really simple, very low coupling, easy to test, the ratio should be much lower. But the idea that there's some target. Oh, well, 1 to 1 is good. Well, no, it all depends. Sometimes you need 10 to 1, and sometimes you need 1 to 10. So, David, you had kind of a list of questions?

DHH: Yeah, well, first, I just wanted to sort of chime in on the whole notion of, at least at first, you guys were talking about, well, I have to be able to comment out any piece of code and see if it will test, which implies that 100% coverage, right? Like, otherwise, you wouldn't be able to remove or comment out anything without having tests break. And that's where that line with what couldn't possibly break, where that goes, I think, is very worth exploring. Because getters and setters, OK, maybe that's the easy one. But I find that, in my code, and working with something like Rails, there are lots of declarative statements that I just don't feel like they have not broken for me enough that it's worth, for me, expending time to write tests for them, right? So that's the, and I'm not seeing that nuance in a lot of discussion around the code. Because it certainly fails the nuance of every production line of code should have a failing test first, right? It shouldn't be able to add an association or a validation or something else to a model unless I have that failing test, according to that paradigm. So that's where I just want to push back a little bit in the sense of, oh, well, you shouldn't have overlap. OK, sure, I agree with that. That still implies 100% coverage, right? You shouldn't have 150% coverage. Then you're fine to go to 100%. But oftentimes, I'm quite comfortable with significantly less than 100%.

Martin Fowler: Yeah, I think also it's part of the calibration you do as you're learning how you're working with a particular environment and system. I mean, when it boils down to it, my feeling is you don't have enough tests, or at least not good enough tests, if you can't confidently change the code, feeling that if you screw something up, something's going to go red and told you. That's the sign of too little. The sign of too much is whenever you're changing the code, you feel like you're expending more effort on changing the tests than changing on the code. That's a sign you've got too much. So you want to be in that Goldilocks kind of zone. And the only real way you know whether you're in that Goldilocks zone comes from experience. You begin to learn, these are the things I tend to screw up. And they screw up in ways I can't immediately catch. And therefore, I need to put more attention onto these. And these things, I don't tend to make that mistake, or my team doesn't tend to make that mistake. So therefore, I can be more relaxed about it. And you tune that as you're working in a given environment. I like the, can I comment out a line of code thing, when I'm not sure about my ground, that's a good starting place to start. But as I would work more with a system, then I'd come up with better heuristics.

DHH: Right. And I think that tuning phase, that's the one I'd love to see more of and get more focus on. Because one of the other things I've seen in the debate is that there's a different tuning for a product team and a consulting team. Somebody who's going to hand over a code base to an unknown group of developers, I find have a tendency to be almost paranoid at times. Right. Like, well, they don't know, these could be the crappiest developers in the world that take over this code base after me. I might, I need to bolt everything down to 250% worth of testing. Right. And then when you have a discussion with somebody who's been in that mode for some amount of time, and you come at it from a production team mode with a stable team, low rotation, good knowledge about the quality of the team, you come at it from a much different angle. Like, I can come at it from an angle where I'm saying, well, most of the time, 60% coverage or 80% coverage buys me enough confidence in making changes and catching errors that they'll do. And then, obviously, if you're having that debate with somebody who's handed over the last 10 code bases they've worked on and seen the atrocities that then happen to them afterward, they'll go like, are you crazy? Are you nuts? And I think that at least just introducing that sort of nuance and pressure to it and rejecting these stables of every production line needs a failing test first kind of things, I think, would be a vast improvement.

Kent Beck: I'd say that the discipline of working strictly for test first. 

is a valuable thing to be able to do, but it's not necessarily something that you want to do all the time. But, but once you're able to do it, you have a wider range of options for your workflow. It's kind of, that's the four-wheel drive low of, of programming. If I get into really sticky spots, I know that I can work in that style. There's other styles that optimize other, other aspects of development. But, but I think your point that hadn't entered the conversation yet, and I think is dead on, is, is this question about who's the, who's the audience and, and what are their needs? That's a part of the, of the trade-offs that we hadn't explored yet. And that's absolutely my experience as well.

DHH: Well, let's, let's jump on to, to the next thing, which is this notion of what is more important in the, in the chain of authority. It used to be that sort of documentation, right? Like first, you documented your whole system, and then you had this wonderful approach, and then the code sort of spelled out of that. And, and we rejected that as a, as an industry, as a general purpose approach, at least, right? And then, at least from some quarters, I'm starting to hear that test is actually at the top of the authority chain. Like to, to understand a system, what you should just be able to do is read the test code first, and then whatever the code says is, is secondary. Sort of placing the, the test as the, as the top level of documentation for what the system should or shouldn't do. I find that to be just as spurious as an argument of documentation. I find going to the root and focusing on the code always being the top level of authority has been more helpful for me to understand systems. And I find that that's also the pressure that leads me to leave the code in a better state. And I'll pull in a third point here, and we can talk about both of them at the same time, which is we have the red green refactor loop, right? Well, as I see it, a lot of times is it's almost like it's red, it's green, and then it's parentheses refactor, that the dopamine release that comes from the flow is making the thing go from red to green. It's not the refactor step, the refactor step often, even though it's set together, sounds like, well, sort of have fun, play video games and eat your vegetables. Like, which part of like those things are you going to do more often and more of the time? And that's where I do this. I just think that those two things together, the notion that the tests are the top level of authority and explaining the system and dictating what is right. If the code does not fill whatever tests are doing, the code is wrong. And then combined with the notion that the red green flip is the important part of red green refactor, or at least tends to, that's where the attention tends to gravitate, leaves a lot of times under refactoring, under focus on code clarity, under focus on a lot of these other principles of just having the cleanest possible code page, which is a little bit of a similar topic to what we talked about, about the test induced damage, that when you paint place the test above the code in terms of the authority and the focus, you get a different pressure, you get a pressure that exerts energy towards the test code, and less energy towards the production code. And I think sort of at least exploring sort of those pressures and seeing where they go. I've not seen enough of that happen.

Kent Beck: So I've just went through the experience of throwing away an implementation, keeping the tests and re-implementing from scratch, and for a data structure that I was working on. And I really like that, that position of knowing, okay, if I have these tests working, then I'm going to be confident that the code is working. So I'm going to answer with a question, under what conditions would you rather throw away the tests and just have the code? And under what conditions would you rather throw away the code and just have the tests? And I think that's like, there are different places where each of those things, I would be happier to lose one half of the system. And I think that's going to be different for everybody. So that's maybe a leave this as an exercise for the listeners to think about what, what are the conditions that would, that would change your answer in that kind of situation? I like going to the tests and being able to say, Okay, well, how's this supposed to behave before I get into all the details of what the data structures are, and what the downstream services, just like, what are the inputs, and what are the outputs, and how are they related? I like having that kind of overview before I dive into an implementation. But that's, that's me, for sure. Martin?

Martin Fowler: Yeah, I, the same thing, I certainly helped, in many cases, understand codebases by looking at the tests to figure out this is what it thinks it's doing. I don't see one as dominant over the other. The whole point is that they're a double check on each other. So the issue is that you have a problem when something goes red, because they're mismatched. The basic fundamental thing is that it's a double check mechanism. What was interesting from what David's point was of people putting so much energy into the tests, this kind of like they're more interested in building a test environment than they are in building production code. And yeah, I've sensed that at times of conversations with people, and that would definitely be a bad move in my view. Because in the end, it's, it's what tests are, a means to an end of getting valuable stuff out for the user. And that's, and putting a lot of energy on that would be another way in which you screw that up. As for the red green refactor thing, I look, that's a, that's a progression of skill. That's the way I look at it. Early on, when people are less skillful, they underdo the refactoring. That seems to be a very common thing. And of course, some people will get stuck in that position. And you, as you get more skillful at the cycle, you get to put more and more attention on the refactoring, because let's face it, being able to freely refactor and to be able to make a nice system is, is what is the great benefit of having tests in the first place. I mean, if we weren't doing it, then the test will become a lot less interesting to us. And, and I just must be weird, I guess, because I get my dopamine shots by, when I look at the code, and it ends up being easier to understand. My biggest dopamine shot, by the way, comes when I have something that I think, Oh, this is going to be a bit of work. I haven't seen this code in a while, I can't remember how to do it. And I go in and it's just very, very straightforward. That's the dopamine shot that really gets you high. But unfortunately, that's several steps. And what leads to that is the fact that you paid attention to the code being really clear and well factored. And there's a big difference between the cleaning up and the getting the really big shot.

Kent Beck: And my friend Jeff Eastman had a physical metaphor for this. He talked about, you've got some design and it kind of just doesn't quite go together. And you wiggle this and you wiggle that. And then at some point, you make one more small change, and it goes foof. And then, and then you look at that thing, and you realize, Oh, this, let me wiggle, wiggle, wiggle, wiggle, and then it goes foof again. And the great days as a designer are the days when you just have this cascade of, this is simpler. No, no, no, no, it's even simpler than that. It's even simpler than that. And I get a huge, huge rush out of those days. It might happen once every six months, that there's just this cascade of, Oh, God, and yes, and now this too, and that, and I'm just laughing my head off. And so I think, I think it is an acquired taste. And that can certainly be overdone, that can become addictive. And you can think, Well, you know, I don't like this position where my code's kind of stinky. So I'm just gonna clean up everything as soon as there's any kind of mess and stop making forward progress at all. I think there's actually a lot of value in increasing the the ambiguity in a code base. If you don't know how to how to clean up a mess, sometimes making more mess is your quickest path to resolution. On the other hand, it's really easy to explain to somebody else the value of, Okay, I have another test running. And it's hard to explain the value to somebody else. Okay, yeah, the same tests all still run, but I've done a bunch more work. And, and, and well, I mean, there's, there's good reasons to do it may create more options for you moving forward, it's easier to, to grow the team or you have fewer locations for defects. But that's just a harder sell to say, Okay, I made progress on the design.

DHH: And I think that one of the reasons it is a harder sell, and one of the other sort of pressures or troubles I see a time for TDD is that it's, it can't be reduced to a number, there's a lot of testing measurements that can be reduced to a number of speed can be reduced to a number, coverage can be reduced to a number, ratio can be reduced to a number. And I think as humans, it's so ever tempting to focus on the things that you can quantify, and quantifying the quality of a design, it's really hard, it's subjective, it's all of these nasty things that scientists, the computer scientists in us, perhaps are less inclined to focus on, because we do have these other things that are measurable. And that's, I see that that's sometimes where things to go astray is focusing on these things that are measurable, but are actually not. I'm not going to say not important, because they are important. They're just so ever, they're not even secondary on the list of things that I care about, when I'm developing a system, test speed, coverage and ratio are quite far down the list. Inherent design and clarity of the system quite high, even higher than that, developing a system that's actually valuable to a user even higher than that. And I just, I worry sometimes that it's a honey trap, that the speed, coverage, ratio, quantifiable state of things, just it's so alluring, that people get sucked into that, and they never come out on the other side. Of course, it doesn't always happen. This is why we talked about pressures and nudging is not about ultimate declarations. But at least being aware that these things are siren calls, that it is possible to get sucked into this makes you more aware that, oh, hey, wait a minute, what are we doing here? Why are we doing this? Maybe this is not the most important things we could chase. And I think the companion to that is this over focus on the whole testing infrastructure. What really gets my goat is stuff like Cucumber, and to a lesser extent, stuff like RSpec, which I think is this glorification of the testing environment to the detriment of the code itself and the focus where that should be. I mean, this is especially around Cucumber. I really have reserved a special place in hell for anybody who forces other people to use Cucumber, except outside of this very narrow, I think, largely imaginary sweet spot where you're writing tests in companion with non-technical stakeholders, which I basically think we have a greater chance of catching unicorns than seeing much in the wild. And all of these things, I think, are symptoms of the same thing, that TDD is a very addictive practice that can suck us into all these things. It doesn't have to. It also can have wonderful good aspects of it. It can have the glory and the good side. And I think that that side has been explored over the past decade in immense detail. The dark side of things, the honey traps and the siren calls, they have hardly even been spoken about. And I think that that's part of the unfortunate nature of when you're trying to sell something up front. I have the same thing with Rails. When you're trying to sell something new that's breaking existing paradigms and trains of thought, you sell things from a very positive angle, right? Like most salesmen do, and we're all salesmen when we're advocating ideas, we position things in the best possible light. Then the switchover that's then supposed to happen is once this sale is made, which for TDD it certainly has been, like that sale, I've seen for as a general accepted principle, TDD is up there. It's not the same battle it was a decade ago. Then you have to switch over and say, okay, we won at least a very large percentage of the battles. Now we have to explore it and we have to be willing to reveal sort of the darker side, siren calls and all these other negative aspects.

Kent Beck: Isn't this a lovely conversation to be having that we were too certain of the behaviors of our systems? 

DHH: Absolutely. I think that's why TDD has conquered all to such an extent that I think that's also why it's hard to switch. It's hard to switch the conversation now because it's been so successful and it's brought so much good to it. When something has done that, you feel like you owe it sort of a place on a pedestal where perhaps that's not the most useful place for it to be once you're trying to go beyond, okay, we won the initial part of enlightenment, right? We have enlightenment. Okay, what's the next step? You can't fight yesterday's battle, right? It's not the same battle anymore, at least it's a travel's eye circle. And I understand TDD is not sold everywhere. And there are plenty of people who still have to deal with those initial battles that are very much in the vein of how things were 10 years ago. Okay, fine, use the same tactics. I'm in the sort of a different era where, as I said, there was a whole generation of programmers who've grown up with this battle as one already, right? So it's a different battlefield now and we have to deal with it in a different way.

Martin Fowler: I'm not sure the battle is run already. I mean, I certainly still hear of plenty of places where we're struggling to get people to do, not necessarily just TDD, but the kind of testing that you're completely happy with. I mean, Rails was designed from the beginning with testing being a part of the thing. I mean, it's right there. It is a framework designed to be testable. Even taking out the TDD side of the question, that focus on testability is still not the common case from what I gather from the people I talk to. I mean, the thing is we're in a fortunate world where we see a lot more of the testing than not. And we have to bear that kind of thing in mind as well. I mean, I agree with you that, yes, you have to get a better nuanced sense of the trade-offs and things, and that's one thing that we hope to do. But don't necessarily think that it's all where you see it is the case everywhere, because there's still a lot of places where it isn't. And we are running out of time, and I'm supposed to be time-cop, aren't I? So, on that note, the next episode we're actually going to have some Q&A. We're going to enable the infamous Q&A app and respond to some questions. We will enable it in before time, so you can place questions before the next session and vote on them, and the most popular questions will be the ones that we'll take more seriously as which ones to answer. Although, in the end, it will be our choice based on what we think is interesting. And we've also agreed that the next one's going to be next Wednesday. So you'll see the announcements. We'll come through our Twitter stream. They'll be on the page that I've put up for this conversation, and we'll have all the details there. So questions and answers next time, with an extra-long episode as well, so that we can waffle on even longer than usual. Thanks a lot.

DHH: Bye-bye.

Kent Beck: Bye-bye.
