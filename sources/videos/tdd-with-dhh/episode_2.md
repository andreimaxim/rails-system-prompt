**Martin Fowler**: Okay, so it looks like we're live, at least if my indicators say anything. And in this case, welcome to the second of the conversations about is TDD dead and with myself, David and Kent. So in the last part, we talked about our different experiences of TDD, about how sometimes it gives us great flow, sometimes not, and differently for different people, me and Kent find that flow more often than David does. We also talked about how TDD and the notion of a self-testing regression suite get conflated together and that they are actually different things, TDD being one way to get there. But we finished the part by really looking at a different issue, which is can at times TDD lead to actual damage to design? And one of the things that TDD proponents like myself often say is that TDD can actually lead you to discover good designs. But David has seen things in the rails world that he feels are the opposite of that, that of damage. So really we want to look at, see if we can get around to three questions during this half hour segment. One is, is TDD the cause of this damage that David observes? Secondly, is it actually damage? Is it a bad thing or a good thing? Because there's clearly disagreement about that. And then thirdly, perhaps the meta question of how can we tell whether something is damaged or not? How do we judge that? So since David is the one that observed the damage, maybe you could describe a little bit of what it is you see as damage. Maybe that's a good place to start.

**DHH**: Absolutely. We said that in the show notes. I think there's a gist. So let me actually see if I can pull that up, because I think I can. Just one second. Yep. Okay. You guys see that? Yeah, so that's the gist. You can go to it in your browser if you want to have a look at it. And it basically just shows one example of one kind of architecture that I've seen people arrive at, either directly in this form or a version of this form, by going through TDD with mockist glasses, if you want to call it that, where the idea is that at every layer of the application, you can separate it completely from any of the other real pieces. So you can take a controller and an MVC set up, and you can figure out a way to test what it does without actually having to talk to the real model, and without having that real model talk to the real database, and without even having the controller run through its normal loop of dealing with the request and the response. We can mock all that stuff out. And it's done through sort of a couple of patterns. At the very top, you see there's the standard Rails controller. This is all extracted from a talk by Jim Weirich called Decoupling from Rails, where he just presented these ideas as a path to go down. And he then looks at how you, first of all, extract the meat of the controller out, the business logic, into this create runner. You extract the model itself out into two pieces, first a repository of how to get and save these models, and then also a business model, as he calls it, that wraps the sort of the data access of the active record. And it's not really, I've seen some criticism about, well, this is just a hello world example. Of course it's going to look far more complicated when you wrap it in a hexagon pattern. And that's true. That's not really what matters. What matters to me, and when I look at test-induced design damage, and when I sort of surface that damage, is the creation of unnecessary indirection, additional layers of complexity that we inject into the application for the purpose of testing it easier. And when I say testing it easier, I generally mean to unit test it without collaboration from other parts of the application. We can separate everything else out and say, I just want to test this tiny little piece as a unit test. It can't be slow, it can't talk to the database, it can't go through the request-response cycle, it can't do any of those things. And this is actually, this is a reasonable extension of that to its logical end. Taking every single piece of an MVC application and slicing it enough times until every single slice can be unit tested in that traditional no-collaboration form of unit testing. And I've seen a variety, I've seen a few applications that's gone the whole way, where they've used both the command pattern to extract the action itself, the repository, and the business wrapper. This shows all of them at once. I've seen many others take just one or two of them. Oh, well, we're just going to inject the command pattern into every controller, such that we can test it in isolation. For me, each slice, whether it's the command pattern, whether it's the repository, whether it's the domain logic wrapper around a data access object, for me, all these slicings are damage. I see an application that's worse by the end of this application, of the patterns. And I see that we're getting there through TDD. And again, as we talked about last time, TDD, there's a lot of different directions. And the direction I'm seeing here is the mockist TDD, somebody who wants to mock out sort of all these collaborators when they're testing just the one thing. And last time, I felt we were actually, we were all in agreement with, oh, well, you shouldn't do three level deeps of mocking. My position is actually starker than that. I don't want to do even one degree of mocking internally in my application. I'm fine with mocking externals. If you're talking to a payment gateway, of course, that payment gateway should be mocked out. If I'm trying to test the controller, I don't want to mock out my model. If I'm trying to test my model, I don't want to mock out my database. These are actually first level layers of mocking. And in my opinion, that drive, that drive towards just getting the first level layer of mocking creates this sort of flow to go down this path of getting all these pattern interjected. And then the ultimate conclusion is this comparison, is that you have all these layers of additional indirection because you need one layer in between each of the other layers so that you can mock from there. And I have a hard time looking at something like this and looking at the more complicated cases and not saying this is test-induced damage. The application is worse off for what you try to do to make it easy to unit test it with mocks.

**Kent Beck**: The thing I found interesting about this conversation, the reason I'm excited to have the conversation is because I'm learning so much in the process of talking to you, interacting with other people in between the sessions. And this idea of test-induced damage was something that was a surprise to me. It wasn't something that I'd ever conceived of before. So I wanted to really understand it. Now, David, you know a lot more about driving cars than I do. But here in Oregon, if you get out of the car and you're someplace that you don't want to be, getting a new car isn't going to fix that. And I feel the same way about ascribing to TDD the design decisions that you make. You're making design decisions. You're going to get feedback in some way. And to say that it's TDD that causes it seems like a conflation of cause and effect. It happened to be around. TDD happened to be around. You could design exactly the same code that you showed without any tests at all. Or you could design it writing tests afterwards or clean room or anything else. To me, the issue is more about all of the tricks that the gist displays are good tricks under certain circumstances. And so for me, it's not a question of TDD or not TDD. It's a question of getting an understanding of when each of those design moves is worth the cost and when it isn't. And if somebody goes around and just wants to apply all of the design ideas every time to everything, regardless of the context, you're going to be in deep yogurt whether you write tests or not.

**DHH**: I think that's all fair. What I will say is I don't think it's... I think once you jump on the TDD horse or car or whatever you want to call it, it's going to sort of want to lead you a certain way. As we talked about last time, the good feelings of TDD when you feel like you're in that flow and you can write just a little bit of code and then you can write some tests for it, I think it's most conducive to the cases when you are unit testing in that traditional sense of no collaborators and so forth. When you move that level of testing up to, say, the controller level or the application level, it's no longer telling you that much about your design anymore because it's moving further and further away from the white box of working with a single object to the black box of working with an entire system. And I can completely see how you get from A to B. I can see that if you like that TDD flow, if you like that feedback flow, especially if you like it fast, you're going to want that feeling as much of the time as you can. And I can completely then... It's not a hard mental jump for me to say then if you enjoy that, why wouldn't you try to apply it everywhere? Why wouldn't you try to apply it to all pieces of code you want to write? Well, I think that you do want to do that and people are trying to do that and then they end up in a place where if they knew they were going to end up there when they started, perhaps they wouldn't have. But they didn't, so they did end up there. And they end up with sort of a monstrosity that it didn't start out with an intention of creating this monstrosity when having all this indirection and having all these layers, but it got there one test at a time. One test I wanted to make faster at a time.

**Kent Beck**: So I would say it got there one design decision at a time. TDD puts a certain kind of evolutionary pressure on a design. No question about that. And testability as a principle puts a kind of a pressure on your design. I think there's a reasonable question about grain size and part of what I'm trying to understand in these conversations is it seems like there are a lot of binary positions along analog dimensions and so grain size is one of those. Some people seem to want to go to one end or the other in terms of granularity of how much, what's the delta coverage per test? Some people don't mind covering a whole bunch of implementation decisions with a single test that aren't covered by any other tests. And other people want to nibble nibble like a mouse and just cover one decision at a time per test. Well, there's a continuum and different decisions have different profiles of how often you're going to screw them up. And so neither extreme position makes sense. But I think being aware of the dimension and being able to adapt your style to the actual cost benefit structures facing you at a given time, that's my goal for myself and my students is opening up that kind of awareness. If you wanted to say, hey, don't take the extreme positions, I'm fine with that. But to say that the kind of evolutionary pressure that TDD creates because you're using your designs very frequently from the outside, to say that that somehow automatically leads you to a poor design, like I don't, like that doesn't, I still don't have that connection in my mind. In particular, you say monstrosity. Can you tell me a kind of thing that you want to do to that gist that the structure of it makes hard? Because like if it's just sitting there, who cares? It's when I want to change it that the design actually matters.

**DHH**: To me at least, there's a direct correlation between the size of the code base and the ease with which you can change it. The larger something is, the harder I found it to change it. And especially when you look at something that has multiple layers of indirection that have to be kept in sync somewhat through the time. So if you look at the repository, to biz object, to actual active record object, sort of that chain all the way down, the repository needs to be updated every single time you have a new interaction with your model. Because that's the layer you're going through, so you have to keep that layer in sync somewhat between both what the caller wants and what the callee is able to give. So just that toll booth and that toll you have to pay then every single time you make an update there, that's an example for me of making things harder and making things more costly to change and more costly to understand too. Because I think that that's, at least when I look at a system and I want to change it, the first thing I have to do is I have to understand it. I have to have a clarity of what is this even trying to do. If I can look at something that fits within 10 lines of code, well, I mean, you can write some very complicated programs in 10 lines of code, Pearl Gulf notwithstanding and such, but if that's not your intention, if you're actually writing clear code and if that clear code fits in 10 lines, to me, that's endlessly easier to understand than 60 lines that has three additional layers of indirection that I have to comprehend first. So that makes it so much harder for me first to get that realization of what is the clarity, what is the system trying to do before I can even begin to propose a change to it. So as a general position, my position is that more code to do same things, bad, unless it's part of the sort of inherent complexity of the domain, right? But it's not in this case, right? Even though this is an exaggerated low-error case, the two pieces of code are doing the same. They're trying to accomplish the same business end goal, right? So when you compare it at that level, then more code equals bad code. That's a gross oversimplification, but I think it illuminates why, to me, the cost of every layer of indirection, the cost of every sort of additional wrapper, I see as a very high price to pay, both in terms of understanding and maintenance and evolution and all these things. And I see that TDD is putting these pressures when you're trying to apply TDD as much as you can, which I fully understand, you're not, right? You're very happy to say, well, TDD doesn't fit in this case. I'm just going to bail. I'm not going to TDD this stuff. What I see is a lot of other people sort of, they get addicted to it. TDD is addicting. It's a red-green refactor is an addictive flow, right?

**Kent Beck**: I am the poorest drug dealer on the planet.

**DHH**: I think you're very crafty of that, and I think that the red-green refactor flow is ingenious in that sense, because it does give a lot of good feelings. It releases a lot of dopamine for a lot of programmers. As we talked about last time, that confidence level, when you get into that flow of, oh, well, I have really great confidence in this. I want to try to use this everywhere else. That's where I see it goes off the rails.

**Martin Fowler**: I'm going to disagree completely with that. I don't think this effect has anything to do with TDD per se, because I can imagine myself in this kind of situation using TDD and not going anywhere near the direction of the kind of layers that you're talking about. I think...

**DHH**: On the other hand, it's not entirely independent of the car that you're driving that's getting you into this place. Because I think there is a testing, a desire to make something testable as part of the driver. But what's really happening here is this desire to isolate the various pieces to the degree that it's going. It's the isolation, I think, that's driving you to this spot. Leaving aside the question of whether this is a good spot or a bad spot, I think it's the isolation that's the driver. When people are saying, we want to construct these indirection layers, they're saying, well, we want to isolate ourselves from the database and how we talk to the database and how we, in fact, interact with the rest of Rails. I mean, very explicitly in a couple of places where I've talked either with this approach to hexagonal architecture or other times that people have talked about it, they've said, we want to get less dependent on Rails. We want to be, and in fact, that's the essence of this hexagonal idea is you want somehow your application to be independent of the environment and the world around it. And so I think it's this desire for isolation that's drawing people down this path. And testability is kind of part of that because anything that's more isolated can be tested more easily, supposedly. I'm not entirely by that either. But I think it's the isolation that's the driver.

**Kent Beck**: I will completely agree with that point. But I think people are getting to that. They're seeing isolation as a goal because of TDD, because of unit testing, that something that's isolated is easier to unit test. And the reason I say that is when I watch people talk about hexagonal architectures and similar approaches to creating further isolation, the only justification that makes sense in my book is actually the testing justification because all the other testing or all the other justifications I've heard have been so ludicrous as to be laughable, especially when it comes to the hexagonal approach. The number one I've seen that people pull out is, oh, I could turn this application into a terminal application. Yes, I could use this from the command line. Is that ever going to happen? Maybe it will happen. Maybe you're working on the SEPA flight booking system. And in that particular very narrow case, that is exactly what you want. But to trounce that off as a general example of, oh, look at this. This is why I'm doing it. One other example, the repository. Oh, look, I could swap out my back end such that I'm no longer talking to a database. I'm instead talking to an in-memory store. Or maybe I'm talking to a web service, which is, to me, the worst part of this isolation thing. When you're isolating a piece as though you could swap in any other piece, and the system would carry on as though it never knew whether it was talking to a web service back end or an in-memory store, they have completely different characteristics in all sorts of degrees, like performance and so forth. You do things with an in-memory store that you would never do over a web service. So to think that you can actually isolate yourself away from this stuff is, to me, just a complete pipe dream. And the reason why I think people are willing to put up with this pipe dream is because that that's not the real goal. That's not the real bone they're getting out of it. The real bone they're getting out of it is isolation for testability. And testability in terms of mocking, fast unit tests, all these other things, right? So I don't buy, for the general case, the other justifications that people have put forward for isolation, especially when it comes to the hexagon approach and terminal apps and in-memory versus web services and all the other nonsense that people are trouncing out. Certainly, I like your example of the difference between a web service and an in-memory database because the reliability differences between the two, that's the kind of stuff I deal with all the time. It has a profound effect. You might think you're decoupled, but you're really, really not because in one case, you have to deal with a bunch of failures that just aren't gonna happen otherwise. And if you start with one and move to the other, you're not gonna have the right design for that. So the boundaries between elements aren't hard. They're going to leak into each other to some degree. And the question is, how much are we willing to spend to get how much decoupling between elements? And to me, that's what I'm trying to understand myself as a designer is to see all these things as trade-offs and there's, okay, so I can work this much harder and I can get that much more decoupling. And so your example of the 10 lines instead of the 60 lines, I would see that as a cohesion argument to say, wow, if I'm solving one problem and all the lines of code for solving it are all together, that's highly cohesive, which means it's going to tend to be loosely coupled. Until such time in the future as you realize, oh, no, no, in fact, these five lines, actually there's another way of implementing them and then you're gonna have to break the 10 lines up and create more elements for yourself.

**Martin Fowler**: I think one thing that's interesting there is the relationship between cohesion and coupling, because I think they're actually often in opposition. Like they're presented as though they're this pair that just wonderfully goes together always. I don't think that that's the case. I think oftentimes to get really low coupling, you do damage cohesion. And I think that that's exactly what we're talking about here, that the drive to get this level of isolation, which is low coupling, right, is causing the cohesion to take a hit. And I think that there are often cases where I'm actually willing to trade high coupling to get high cohesion. I'm willing to say this piece of code is, like my controller in an MVC application, my controller will explicitly refer to the model, it'll explicitly refer to some authentication code, it'll explicitly refer to a mailer object that knows how to send out email. All these things are explicit dependencies that can't be easily stopped out. They can't be easily swapped out. I'm hard-coding the class names right in there. So I'm damaging the coupling. I'm willing to suffer high coupling because I also get high cohesion out of it. I get a piece of code that does something in a very succinct space, right? And with that trade-off comes that I'm willing to say, do you know what, that piece of code is not going to be easily unit-testable in that fast test, no collaborators kind of way. It's even going to be a little bit of trick to mock it out. So do you know what, I'm not even going to try. Like, I think that the payoff in that case is that I have that high cohesion, I have a system that's easy to comprehend, a clear system anybody can jump into and see what's going on, and I paid the price. And I think that that's why this discussion to me is interesting. I think it's why at least just naming and calling out and introducing as a concept test-induced damage, that this drive that testing at times has towards isolation, towards low coupling, can be harmful to other aspects of the trade-off. That's not, to me, that shouldn't be as controversial as it's been perceived at least.

**Kent Beck**: Well, I think there's another option, which is that... So the email example, I think, is a great one. You always have the option of reifying intermediate results. Sometimes it's worth the price and sometimes it isn't. Mocking isn't the only way that you can eliminate external dependencies. You can also, and I use the example of a compiler, which goes a lot deeper. I mean, you have the parse tree as an intermediate result, you have a single assignment form as an intermediate result, you have the assembly code as some symbolic form of the assembly code as an intermediate result. In all those cases, the trade-off has really been clear for decades that those intermediate results, objective-ifying, reifying, having those intermediate results as things gives you a lot of leverage. And in the email case, I think there's a missing... One of the options is there's a missing piece of design, which is thinking about how intermediate results as things might make code testable without needing to use mocks. It's one of the options. It needs to be in your bag of tricks. It's subject to a set of trade-offs like everything else, but that's the one that, like, when I have something that's just really hard to test and I take a walk and I chop some wood and I feed some goats and I, oh, I see. The insights that I get in that moment generally are design insights. So difficulty testing is a symptom of poor design. Once I have the design insight, oh, I thought these two things went together and really they go apart or vice versa. I thought these things were really separate and if I put them together, a bunch of things suddenly become easy. Once I have that insight, then I get both. And that's what I'm shooting for. I want testable designs that are easy to comprehend, that are easy to use in different contexts, that are easy to modify. And to me, the gating factor isn't my coding workflow; it's really about how much design insight can I get going in my own head.

**DHH**: I think that that's, of course, the ideal. So the ideal is that the testing drives you towards a better design. And I've seen that. I've had that happen. It happened to me that either through TDD, we test first or test afterwards, that the test told me something about my system that made it better. But I've also had the opposite happen. I don't want an equation between, oh, this is easily testable, thus it's better. I think that that's the fallacy that a lot of poor decisions are currently being driven off. That it's not a trade-off. It's being presented simply as a matter of, if you think about it longer, if you go out with more goats for longer periods of time, the insight will arrive, and you will have this epiphany that's going to improve the design and make it testable. I often will propose that the solution is not there. You cannot both have the easily testable and the good, coherent design. Not always, but there's a variety of cases of this where I think this code that we looked at in the gist is a perfect example of that. Trying to think harder about how the controller could be better structured in such a way that it was easier to test, I think we're just not getting there. And I want to stop people from at least thinking that the only thing holding them back is that they haven't thought about it long enough. I think sometimes the answer is there's not an answer. You're not going to produce better code by making it more testable.

**Kent Beck**: I'm going to say something to you, David, that I think you probably don't hear very often, and I think you don't have enough self-confidence. I think that the design, that there are design insights out there that maybe you can't see them today, but that unlock that win-win situation. Now, you absolutely have to make progress in the meantime. You don't know a good way to design it, so you have to get the next test running or release the next feature however you want to talk about it, even though you know that the design isn't quite right, you know, just smells bad. But I'm going to be optimistic. I'm going to take the optimistic stance as we get close to time today to say, I think that really there is a good design out there. There's some abstraction that we're missing, and if only we could figure it out, then we could have our cake and eat it too. In the meantime, we need to choose.

**DHH**: I think you've crystallized what I call faith-based TDD. This belief that TDD will eventually, even though we cannot see it and we have no evidence for it today, will lead you to the right design. And I think that that's actually, that was the loop that I was in for a while, and that loop just led me to feel bad about myself when I couldn't arrive at these conclusions. Like, the faith in TDD was driving me towards the sort of, it must be out there. I am simply just not smart enough or have not thought about it long enough. There's not, there was not an out. And I think that that's the realization I've come to, and that's why, to me, TDD as a faith-based programming principle is dead, that I cannot anymore just place my faith that TDD eventually is going to lead me to this proper design. I will accept that there are just, for a large number of cases, it's just simply no way that TDD is gonna drive me to the better design, that actually TDD is gonna take me to the wrong place, notwithstanding that there are other cases where it will take me to the right place, but I'm giving up on the faith in it as a sort of a universal solution that it is going to eventually drive me there.

**Kent Beck**: Yeah, I wasn't referring to TDD or not to TDD. I was talking about software design in general first. And second, TDD isn't taking you anywhere. You're making the decisions. You get where you get. If you say, well, these aren't the design decisions I wanted to make, well, we'll make different design decisions. And you, stepping back to the principles, how are you gonna get feedback about the physical behavior of the system you're building? How are you gonna get feedback about the API decisions that you're making? How are you going to feel good? How are you gonna earn that confidence I talked about last time? Nobody's gonna hand that to you. You're gonna have to earn it. How are you gonna earn that? That, to me, is the essence of the question. And I think that thinking about software design, I don't care about TDD, not TDD. Thinking about software design, when I can finally have that insight, that's the moment that pays off, both in terms of dopamine and in terms of externally measurable results. That's the moment that I'm really seeking because it just pays off so big. And I don't think that's tied to workflow at all. I think that's about knowing when to work hard and knowing when to rest and pulling lots of influences in from other places and knowing how to collaborate with other people so that you're not limited to the genius in your own head. Full workflow is way, way down on the list about whether or not you get to that moment that really pays off big for software development. Martin, are you gonna stop us?

**Martin Fowler**: I've been thinking when to join in and be TimeCarp and trying to think of some dramatic thing to move on to the next conversation. But actually, I can't think of a dramatic thing to move on to the next conversation. Have you two got any ideas?

**Kent Beck**: Well, I'd really like to explore this, what are the trade-offs involved? I think that's a really good question. What are the trade-offs involved? I think that's a space that we really haven't talked about. What are the, what's the context that affects things like how you seek out, how frequently and how do you seek out feedback? And I'd love to talk about that. I think there's a ton of juice in there, things that I haven't really thought about and I'm in this for my learning. So I'd love to go there.

**DHH**: I think that that's where the granularity debate can sort of come in and where test first versus test after. And the value, for example, of test first at a higher level of granularity, I'm very unclear on that.

**Martin Fowler**: Okay, so that will be the theme that we'll move on to next time then and Kent can kick us off and head in that direction. But now our time is somewhat past, so it's time to be quiet for a bit and let the internet see what it comes up with over the next week. And as for the next time, we'll post it. It may be next week, but we haven't actually planned it out yet. We've perhaps also planned it out before the video, but oh well, such is life. We'll post it on our feeds. Keep an eye out for it. Bye.

**DHH**: Bye-bye.
